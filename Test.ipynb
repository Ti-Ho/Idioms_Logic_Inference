{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 测试CUDA是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"gpu\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 字符串测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://zaojv.com/4420198.html\"\n",
    "urll = url[0:-5]\n",
    "urlr = url[-5:]\n",
    "print(urll)\n",
    "print(urlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 测试正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def delChars(sentence):\n",
    "    \n",
    "    sentence = re.sub(r'^(（)*(\\()*', \"\", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r'^(\\d+)*(\\.)*(\\))*(）)*(、)*', \"\", sentence)\n",
    "    sentence = re.sub('(\\d+)*([a-zA-Z]+)', \"\", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub('(,)+', \"，\", sentence)\n",
    "    return sentence\n",
    "\n",
    "sentence = \"(1) 这些玻璃工ASFJKDSHFH艺品玲珑asdfaadfa剔透,令人爱不释手。\"\n",
    "sentence = delChars(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = \"刁迈彭一面让他起，一面故意做出～的样子，说‘这是怎么好！这是怎么好！叫我怎么对得起死的大哥！’★李宝嘉《官场现形记》第五十一回\"\n",
    "sentence = re.sub('(★.*)+', \"\", sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 测试循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cy = ['爱不释手', '奇葩异卉', '山清水秀', '美不胜收', '望而生畏']\n",
    "for cyi in range(0, len(cy) - 1):\n",
    "    for cyj in range(cyi + 1, len(cy)):\n",
    "        print(str(cyi) + str(cyj))\n",
    "        print(cy[cyi] + \" \" + cy[cyj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "cy = ['爱不释手', '奇葩异卉', '山清水秀', '美不胜收', '望而生畏']\n",
    "for cyi in range(0, len(cy) - 1):\n",
    "    for cyj in range(cyi + 1, len(cy)):\n",
    "        data = []\n",
    "        data.append(cy[cyi])\n",
    "        data.append(cy[cyj])\n",
    "        data.append(sentence)\n",
    "#         print(data)\n",
    "        datalist.append(data)\n",
    "cy = ['爱不释手2', '奇葩异卉2', '山清水秀2', '美不胜收2', '望而生畏2']\n",
    "for cyi in range(0, len(cy) - 1):\n",
    "    for cyj in range(cyi + 1, len(cy)):\n",
    "        data = []\n",
    "        data.append(cy[cyi])\n",
    "        data.append(cy[cyj])\n",
    "        data.append(sentence)\n",
    "#         print(data)\n",
    "        datalist.append(data)\n",
    "print(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 测试字符串列表去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cy_set = set({\"爱不释手奇葩异卉\"})\n",
    "tmp_set = set({\"爱不释手美不胜收\"})\n",
    "# cy_set = cy_set | tmp_set # 并集\n",
    "cy_set = cy_set & tmp_set # 交集\n",
    "print(cy_set)\n",
    "print(cy_set == set())\n",
    "print(len(cy_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_set = set()\n",
    "subdata_i = ['哀兵必胜', '骄兵必败', '有句话叫做骄兵必败，哀兵必胜。虽然我们队赢了这场比赛，但我们不能骄傲，否则下次输的一方一定是我们。']\n",
    "strcat1 = {subdata_i[0] + subdata_i[1]}\n",
    "strcat2 = {subdata_i[1] + subdata_i[0]}\n",
    "print(strcat1)\n",
    "print(strcat2)\n",
    "all_set |= strcat1\n",
    "all_set |= strcat2\n",
    "print(all_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试写csv文件\n",
    "pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#任意的多组列表\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]    \n",
    "\n",
    "#字典中的key值即为csv中列名\n",
    "dataframe = pd.DataFrame({'a_name':a,'b_name':b})\n",
    "，,\n",
    "#将DataFrame存储为csv,index表示是否显示行名，default=True\n",
    "dataframe.to_csv(\"test.csv\",index=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = [[1, 2, 3],[2, 3, 4],[3, 4, 5]]\n",
    "a1 = pd.DataFrame(a)\n",
    "a1.to_csv('test.csv',mode='a', index=False,header=[\"成语1\", \"成语2\", \"造句\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#python2可以用file替代open\n",
    "with open(\"test.csv\",\"a\", newline = \"\") as csvfile: \n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    #先写入columns_name\n",
    "    writer.writerow([\"index\",\"a_name\",\"b_name\"])\n",
    "    #写入多行用writerows\n",
    "    writer.writerows([[0,1,3],[1,2,3],[2,3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试读csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# data = pd.read_csv('DataCrawler/MyData/Data_1.csv')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算爬取的总数据量（条）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"DataCrawler/MyData/Data_.csv\"\n",
    "tot_len = 0\n",
    "for i in range(1, 29):\n",
    "    pathl = filepath[0:-4]\n",
    "    pathr = filepath[-4:]\n",
    "    nowpath = pathl + str(i) + pathr\n",
    "    # print(nowpath)\n",
    "    data = pd.read_csv(nowpath)\n",
    "    tot_len += len(data)\n",
    "    print(data)\n",
    "print(tot_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"1.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 测试清理脏数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subPageData = [['爱不释dsfa', '奇葩异卉', '这些玻璃工艺品玲珑剔透，令人爱不释手。'],['爱不释手', '奇葩异卉', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['爱不释手', '山清水秀', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['爱不释手', '美不胜收', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['爱不释手', '望而生畏', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['奇葩异卉', '山清水秀', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['奇葩异卉', '美不胜收', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['奇葩异卉', '望而生畏', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['山清水秀', '美不胜收', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['山清水秀', '望而生畏', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['美不胜收', '望而生畏', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['爱不释手2', '奇葩异卉2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['爱不释手2', '山清水秀2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['爱不释手2', '美不胜收2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['爱不释手2', '望而生畏2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['奇葩异卉2', '山清水秀2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['奇葩异卉2', '美不胜收2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['奇葩异卉2', '望而生畏2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['山清水秀2', '美不胜收2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['山清水秀2', '望而生畏2', '这些玻璃工艺品玲珑剔透，令人爱不释手。'], ['美不胜收2', '望而生畏2', '这些玻璃工艺品玲珑剔透，令人爱不释手。']]\n",
    "datalist = []\n",
    "for subdata_i in subPageData:\n",
    "    if len(subdata_i[0]) < 4 or len(subdata_i[1]) < 4:\n",
    "        continue\n",
    "    datalist.append(subdata_i)\n",
    "print(len(datalist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 使用pandas将csv文件转成xlsx文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def csv_to_xlsx_pd():\n",
    "    csv = pd.read_csv('DataCrawler/MyData/Data_4.csv', encoding='utf-8')\n",
    "    csv.to_excel('1.xlsx', sheet_name='data', index = False, header = None)\n",
    "csv_to_xlsx_pd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 使用第三方库pandas将xlsx文件转csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def xlsx_to_csv_pd():\n",
    "    data_xls = pd.read_excel('1.xlsx', index_col=0)\n",
    "    data_xls.to_csv('1.csv', encoding='utf-8')\n",
    "\n",
    "xlsx_to_csv_pd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 测试json、python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'阿鼻地狱': {'explanation': '阿鼻梵语的译音，意译为无间”，即痛苦无有间断之意。常用来比喻黑暗的社会和严酷的牢狱。又比喻无法摆脱的极其痛苦的境地。', 'example': '但也有少数意志薄弱的……逐步上当，终至堕入～。★《上饶集中营·炼狱杂记》'}, '阿党比周': {'explanation': '指相互勾结，相互偏袒，结党营私。', 'example': '《论语·卫灵公》众恶之，必察焉；众好之，必察焉”何晏集解引三国魏王肃曰或众～，或其人特立不群，故好恶不可不察也。”'}, '阿党相为': {'explanation': '阿党偏袒、偏私一方。为了谋求私利相互偏袒、包庇。', 'example': '无'}, '阿狗阿猫': {'explanation': '旧时人们常用的小名。引申为任何轻贱的，不值得重视的人或著作。', 'example': '无'}, '阿姑阿翁': {'explanation': '阿名词的前缀。姑丈夫的母亲。翁丈夫的父亲。指公公婆婆。', 'example': '既然如此，你我两个，便学个不痴不聋的～。★《儿女英雄传》二三回'}}\n",
      "<class 'dict'>\n",
      "['阿鼻地狱', '阿党比周', '阿党相为', '阿狗阿猫', '阿姑阿翁']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "idiomDict = {}\n",
    "# 读取test.json中的数据\n",
    "with open('test.json', 'r',encoding=\"utf-8\") as f1:\n",
    "    list1 = f1.readlines()[0]\n",
    "    preData = json.loads(list1)\n",
    "    for data in preData:\n",
    "        word = data[\"word\"]\n",
    "#         print(data[\"explanation\"])\n",
    "#         print(data[\"example\"])\n",
    "        idiomDict[word] = {}\n",
    "        idiomDict[word][\"explanation\"] = data[\"explanation\"]\n",
    "        idiomDict[word][\"example\"] = data[\"example\"]\n",
    "\n",
    "print(idiomDict)\n",
    "print(type(idiomDict))\n",
    "print(list(idiomDict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 测试CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 2, 0, 2, 1, 1, 1, 0, 0, 1, 2, 1, 2, 1, 2])\n",
      "tensor([[ 0.4870,  2.0264,  1.9457],\n",
      "        [-0.8106, -0.4189, -0.2331],\n",
      "        [ 0.8117,  0.1031, -0.5369],\n",
      "        [-0.4613, -0.1268, -0.0186],\n",
      "        [ 0.6888,  0.2111,  0.1638],\n",
      "        [ 0.0987,  1.4254,  0.1458],\n",
      "        [ 0.2610,  0.9352,  1.7628],\n",
      "        [-0.6567, -0.1876, -0.4520],\n",
      "        [ 0.2947,  0.2893, -0.6391],\n",
      "        [ 1.2170,  0.7614, -0.4177],\n",
      "        [-0.6920, -0.4242, -0.1227],\n",
      "        [ 1.7133,  0.3522, -0.9335],\n",
      "        [-0.3449, -0.6127, -0.8734],\n",
      "        [ 0.0263, -0.1226, -0.7970],\n",
      "        [ 0.6767,  2.1075, -0.9630],\n",
      "        [-0.2614, -0.3616,  0.8290]], grad_fn=<AddmmBackward>)\n",
      "tensor(1.1381, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(10, 3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "x = torch.randn(16, 10)\n",
    "# print(x)\n",
    "y = torch.randint(0, 3, size=(16,))  # (16, )\n",
    "print(y)\n",
    "logits = model(x)  # (16, 3)\n",
    "print(logits)\n",
    "loss = criterion(logits, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看训练日志文件\n",
    "* 获取train_loss 用于ECHARTS折线图的data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch  train_loss  train_acc test_loss  test_acc\n",
      "0      0    1.037369   0.464894  1.033418  0.502797\n",
      "1      1    0.999544   0.499818  1.027154  0.503954\n",
      "2      2    0.991576   0.503632  1.026418  0.506654\n",
      "3      3    0.987847   0.502946  1.024737  0.509547\n",
      "4      4    0.985253   0.504553  1.022145  0.509932\n",
      "5      5    0.982536   0.505903  1.019545  0.511475\n",
      "6      6    0.979560   0.508238  1.016818  0.512247\n",
      "7      7    0.977170   0.509995  1.013082  0.513211\n",
      "8      8    0.974004   0.509502  1.010937  0.514176\n",
      "9      9    0.971730   0.511795  1.007149  0.517454\n",
      "10    10    0.968525   0.513873  1.001709  0.518419\n",
      "11    11    0.965949   0.514944  0.998334  0.521311\n",
      "12    12    0.962778   0.517237  0.994096  0.522469\n",
      "13    13    0.960138   0.518030  0.988279  0.524783\n",
      "14    14    0.957496   0.518672  0.983799  0.526712\n",
      "15    15    0.954009   0.522786  0.979218   0.52864\n",
      "16    16    0.950479   0.523900  0.974493  0.530955\n",
      "17    17    0.948693   0.523600  0.971217  0.530955\n",
      "18    18    0.946542   0.525379  0.966687  0.532498\n",
      "19    19    0.943849   0.528421  0.963362  0.532305\n",
      "20    20    0.940378   0.529514  0.960285  0.534041\n",
      "21    21    0.940465   0.526921  0.957055  0.535198\n",
      "22    22    0.938915   0.531142  0.955511  0.535583\n",
      "23    23    0.936198   0.532213  0.952671  0.534812\n",
      "24    24    0.935073   0.532064  0.950778  0.536355\n",
      "25    25    0.933916   0.533820  0.948443  0.536933\n",
      "26    26    0.932417   0.532985  0.947244  0.536933\n",
      "27    27    0.931369   0.534142  0.946391  0.537512\n",
      "28    28    0.930920   0.534142  0.944629  0.538284\n",
      "29    29    0.928686   0.535684  0.943156  0.537705\n",
      "30    30    0.928330   0.535599  0.941343  0.538476\n",
      "31    31    0.926979   0.536584  0.940582  0.538284\n",
      "32    32    0.926956   0.537356  0.938017  0.539248\n",
      "33    33    0.924835   0.537120  0.937287  0.540019\n",
      "34    34    0.925201   0.536284  0.936251  0.539055\n",
      "35    35    0.924307   0.538920  0.934568  0.540405\n",
      "36    36    0.923823   0.539327  0.933847  0.540598\n",
      "37    37    0.923516   0.536477   0.93346  0.540598\n",
      "38    38    0.923682   0.537206   0.93292  0.541176\n",
      "39    39    0.922061   0.538384  0.933109  0.541369\n",
      "40    40    0.921836   0.540141  0.932024  0.541755\n",
      "41    41    0.922437   0.538470   0.93181  0.542719\n",
      "42    42    0.921403   0.541684  0.930893  0.543877\n",
      "43    43    0.921248   0.537741  0.930395  0.543298\n",
      "44    44    0.919753   0.539927  0.929421  0.543105\n",
      "45    45    0.919789   0.539070   0.92788  0.543491\n",
      "46    46    0.919076   0.538491  0.926805  0.543877\n",
      "47    47    0.919918   0.540677  0.926633  0.543105\n",
      "48    48    0.919379   0.539198  0.925953  0.543491\n",
      "49    49    0.917960   0.540698  0.924986  0.542912\n",
      "50    50    0.918332   0.541255  0.924438  0.543298\n",
      "51    51    0.919379   0.540034  0.923844  0.543105\n",
      "52    52    0.917830   0.540398  0.923416  0.543298\n",
      "53    53    0.918672   0.540248  0.923176  0.543491\n",
      "54    54    0.918166   0.541062  0.922949  0.543491\n",
      "55    55    0.918136   0.541105  0.922992  0.543684\n",
      "56    56    0.918589   0.541705   0.92297  0.543684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.0373688 , 0.99954391, 0.99157605, 0.98784667, 0.98525307,\n",
       "       0.98253595, 0.9795602 , 0.97716979, 0.97400375, 0.9717297 ,\n",
       "       0.96852497, 0.96594896, 0.96277808, 0.96013765, 0.95749585,\n",
       "       0.95400866, 0.95047928, 0.94869281, 0.94654232, 0.94384885,\n",
       "       0.94037837, 0.9404654 , 0.93891471, 0.93619817, 0.93507252,\n",
       "       0.9339158 , 0.93241709, 0.93136947, 0.93092031, 0.92868649,\n",
       "       0.92832957, 0.92697858, 0.9269564 , 0.92483477, 0.92520076,\n",
       "       0.92430654, 0.92382296, 0.92351595, 0.92368185, 0.92206069,\n",
       "       0.92183608, 0.92243669, 0.92140265, 0.92124816, 0.919753  ,\n",
       "       0.9197892 , 0.91907641, 0.91991825, 0.91937882, 0.91795984,\n",
       "       0.9183317 , 0.91937876, 0.91783003, 0.91867194, 0.91816645,\n",
       "       0.91813608, 0.91858871])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_pickle('IdiomBertModel/state_dict/multi__state_dict/df_log.pickle')\n",
    "print(data)\n",
    "np.array(data.train_loss.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 获取epoch用于ECHARTS折线图的xAxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epoch0' 'epoch1' 'epoch2' 'epoch3' 'epoch4' 'epoch5' 'epoch6' 'epoch7'\n",
      " 'epoch8' 'epoch9' 'epoch10' 'epoch11' 'epoch12' 'epoch13' 'epoch14'\n",
      " 'epoch15' 'epoch16' 'epoch17' 'epoch18' 'epoch19' 'epoch20' 'epoch21'\n",
      " 'epoch22' 'epoch23' 'epoch24' 'epoch25' 'epoch26' 'epoch27' 'epoch28'\n",
      " 'epoch29' 'epoch30' 'epoch31' 'epoch32' 'epoch33' 'epoch34' 'epoch35'\n",
      " 'epoch36' 'epoch37' 'epoch38' 'epoch39' 'epoch40' 'epoch41' 'epoch42'\n",
      " 'epoch43' 'epoch44' 'epoch45' 'epoch46' 'epoch47' 'epoch48' 'epoch49'\n",
      " 'epoch50' 'epoch51' 'epoch52' 'epoch53' 'epoch54' 'epoch55' 'epoch56']\n",
      "['epoch0' 'epoch1' 'epoch2' 'epoch3' 'epoch4' 'epoch5' 'epoch6' 'epoch7'\n",
      " 'epoch8' 'epoch9' 'epoch10' 'epoch11' 'epoch12' 'epoch13' 'epoch14'\n",
      " 'epoch15' 'epoch16' 'epoch17' 'epoch18' 'epoch19' 'epoch20' 'epoch21'\n",
      " 'epoch22' 'epoch23' 'epoch24' 'epoch25' 'epoch26' 'epoch27' 'epoch28'\n",
      " 'epoch29' 'epoch30' 'epoch31' 'epoch32' 'epoch33' 'epoch34' 'epoch35'\n",
      " 'epoch36' 'epoch37' 'epoch38' 'epoch39' 'epoch40' 'epoch41' 'epoch42'\n",
      " 'epoch43' 'epoch44' 'epoch45' 'epoch46' 'epoch47' 'epoch48' 'epoch49'\n",
      " 'epoch50' 'epoch51' 'epoch52' 'epoch53' 'epoch54' 'epoch55' 'epoch56']\n"
     ]
    }
   ],
   "source": [
    "xAxis = np.array(data.epoch.values)\n",
    "for i, x in enumerate(xAxis):\n",
    "    xAxis[i] = 'epoch' + str(x)\n",
    "print(xAxis)    \n",
    "print(np.array(xAxis))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env4bert]",
   "language": "python",
   "name": "conda-env-env4bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
